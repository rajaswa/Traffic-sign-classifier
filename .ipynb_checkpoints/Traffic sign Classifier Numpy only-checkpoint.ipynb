{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'train.p'\n",
    "validation_file= \"test.p\"\n",
    "testing_file = \"valid.p\"\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#converting to numpy arrays\n",
    "\n",
    "xtrain = np.array(X_train)\n",
    "ytrain = np.array(y_train)\n",
    "xvalid = np.array(X_valid)\n",
    "yvalid = np.array(y_valid)\n",
    "xtest = np.array(X_test)\n",
    "ytest = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 4410\n",
      "Image data shape = 1024\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "# TODO: Number of training examples\n",
    "n_train = np.shape(ytrain)[0]\n",
    "\n",
    "# TODO: Number of validation examples\n",
    "n_validation = np.shape(yvalid)[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = np.shape(ytest)[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = 32 * 32\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = 43\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5VRFkLNUAnYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27c3372d630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing data samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "imgplot = plt.imshow(xtrain[8000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing functions\n",
    "\n",
    "def normalize(x):\n",
    "    return (x.astype(float) - 128) / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "\n",
    "xtrain_norm = normalize(xtrain)\n",
    "xtest_norm = normalize(xtest)\n",
    "xvalid_norm = normalize(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = 1 / (1+np.exp(0-Z))\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, \n",
    "                                             parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)], \n",
    "                                             activation='relu')\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, \n",
    "                                          parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \n",
    "                                          activation='sigmoid')\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1-Y), np.log(1-AL))) / m * (-1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ, cache[0].T ) / m\n",
    "    db = (np.sum(dZ, axis=1, keepdims=True)) / m\n",
    "    dA_prev = np.dot(cache[1].T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    \n",
    "        \n",
    "    if activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[1]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_backward(sigmoid_backward(dAL, current_cache[1]), current_cache[0])\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 34799\n",
      "Number of testing examples: 4410\n",
      "Each image is of size: (32, 32, 3)\n",
      "train_x_orig shape: (34799, 32, 32, 3)\n",
      "train_y shape: 34799\n",
      "test_x_orig shape: 4410\n",
      "test_y shape: 4410\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = xtrain.shape[0]\n",
    "num_px = xtrain.shape[1]\n",
    "m_test = xtest.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(xtrain.shape))\n",
    "print (\"train_y shape: \" + str(ytrain.shape[0]))\n",
    "print (\"test_x_orig shape: \" + str(xtest.shape[0]))\n",
    "print (\"test_y shape: \" + str(ytest.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (3072, 34799)\n",
      "test_x's shape: (3072, 4410)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = xtrain.reshape(xtrain.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = xtest.reshape(xtest.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 32 * 32 * 3    # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, 'sigmoid')\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'sigmoid')\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-197-d868b8a785ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwo_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-196-7048de3602f9>\u001b[0m in \u001b[0;36mtwo_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m### START CODE HERE ### (≈ 2 lines of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mA1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mA2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m### END CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-186-208590d55543>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m### START CODE HERE ### (≈ 2 lines of code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m### END CODE HERE ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, ytrain, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
